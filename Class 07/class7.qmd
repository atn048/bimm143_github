---
title: "Class 07: Machine Learning 1"
author: "Amy Nguyen (PID: A18148284)"
format: pdf
toc: true
---

## Background

Today we will be begin our re-exploration of important machine learning methods with a focus on **clustering** and **dimensionality reduction**

To start testing these methods let's make up some sample data to cluster where we know what the answer should be.

```{r}
hist(rnorm(3000, mean=10))
```

> Q. Can you generate 30 numbers centered at +3 taken at random from a normal distribution?

```{r}
tmp <- c(rnorm(30, mean=3), 
         rnorm(30, mean=-3) )

x <- cbind(x=tmp, y=rev(tmp))
plot(x)
```

## K-means clustering 

The main function in "base R" for K-means clustering is called `kmeans()`, lets try it out:

```{r}
k <- kmeans(x, centers = 2)
k 
```
> Q. What component of your kmeans result object has the cluster centers? 

```{r}
k$centers
```

> Q. What component of your kmeans result object has the cluster size (i.e. how many points are in each cluster)? 

```{r}
k$size
```


> Q. What component of your kmeans result object has the cluster membership vector (i.e. the main clustering result: which points are in which cluster)? 

```{r}
k$cluster
```

> Q. Plot the results of clustering (i.e. our data colored by the clustering result) along with the cluster centers.

```{r}
plot(x, col=k$cluster)
#points(k$centers, col="blue", pch=15, cex=2)
```
> Q. Can you run kmeans again and cluster into 4 clusters and plot the results just like we did above with coloring by cluster and the cluster centers shown in blue?

```{r}
plot(x, col=k$cluster)
points(k$centers, col="blue", pch=15)
```


> Q.  Run kmeans() again and this time produce 4 clusters and call your result object 'k4' and make a results figure like above?

```{r}
k4 <- kmeans(x, center = 4)
```
```{r}
plot(x, col=k4$cluster)
points(k$centers, col="blue", pch=15, cex=2)
```
> **Key-point:** Kmeans will always return the clustering that we ask for (this is the "K" or "centers" in K-means)!

```{r}
k$tot.withinss
```

## Hierarchial clustering 


The main function for hierarchical clustering in base R is called `hclust()`. One of the main differences with respect to the `kmeans()` function is that you can not just pass your input data directly to `hclust()` - it needs a "distance matrix" as input. We can get this from lots of places including the `dist()` function. 


```{r}
d <- dist(x)
hc <- hclust(d)
plot(hc)
```

We can "cut" the dendrogram or "tree" at a given height to yield our "clusters". For this we use the function `cutree()`

```{r}
plot(hc)
abline(h=4.5, col="red")
grps <- cutree(hc, h=10)
```

```{r}
grps
```

> Q. Plot our data `x` colored by the clustering result from `hclust()` and `cutree()`?

```{r}
plot(x, col=grps)
```
```{r}
plot(hc)
abline(h=5.6, col="red")
grps <- cutree(hc, h=5.6)
```

## Principal Component Analysis (PCA) 

PCA is a popular dimensional reduction technique that is widely used in bioinformatics.

### PCA of UK food data

Read data on food consumption in the UK

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
x
```

It looks like the row names are not set properly. We can fix this.

```{r}
rownames(x) <- x[,1]
x <- x[,-1]
x
```

A better way to do this is fix the row names assignment at import time:

```{r}
read.csv(url, row.names = 1)
```

> Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions? 17 rows, and 4 colums.

```{r}
dim(x)
```

> Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?
I prefer the read.csv(url, row.names = 1)

> Q3: Changing what optional argument in the above barplot() function results in the following plot? Changing beside=T to beside=F, or deleting it.

```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```

```{r}
library(tidyr)

# Convert data to long format for ggplot with `pivot_longer()`
x_long <- x |> 
          tibble::rownames_to_column("Food") |> 
          pivot_longer(cols = -Food, 
                       names_to = "Country", 
                       values_to = "Consumption")
dim(x_long)
```

> Q4: Changing what optional argument in the above ggplot() code results in a stacked barplot figure? Change dodge to stack 

```{r}
library(ggplot2)

ggplot(x_long) +
  aes(x = Country, y = Consumption, fill = Food) +
  geom_col(position = "stack") +
  theme_bw()
```

> Q5: We can use the pairs() function to generate all pairwise plots for our countries. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot? A point on the diagonal of a given plot represents a value where the variable on the x-axis is equal to the variable on the y-axis. Each country is being compared to/plotted against itself, so the off-diagnoal plots show relationships between different countries. 

```{r}
pairs(x, col=rainbow(nrow(x)), pch=16)
```

### Heatmap

We can instaill the **pheatmap** package with the `install.packages()` command that we used previously. Remember that we always run this in the console and not a code chunk in our quarto document. 

> Q6. Based on the pairs and heatmap figures, which countries cluster together and what does this suggest about their food consumption patterns? Can you easily tell what the main differences between N. Ireland and the other countries of the UK in terms of this data-set? England, Scotland, and Wales cluster together, suggesting similar food consumption patterns. Based on the color intensity of the heat map, it can be observed that N.Ireleand has higher consumption of fresh fruit, fresh potatoes while lower consumption of alcoholic drinks compared to the other UK countries.

```{r}
library(pheatmap)
pheatmap( as.matrix(x) )
```

Of all these plots really only the `pairs()` plot was useful. This however took a bit of work to interpret and will not scale when I am looking at much bigger data sets.

## PCA the rescue

The main function in "base R" for PCA is called `prcomp()`.  


```{r}
pca <- prcomp( t(x) )
summary(pca)
```

> Q7. Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.

```{r}
# Create a data frame for plotting
df <- as.data.frame(pca$x)
df$Country <- rownames(df)

# Plot PC1 vs PC2 with ggplot
ggplot(pca$x) +
  aes(x = PC1, y = PC2, label = rownames(pca$x)) +
  geom_point(size = 3) +
  geom_text(vjust = -0.5) +
  xlim(-270, 500) +
  xlab("PC1") +
  ylab("PC2") +
  theme_bw()
```
> Q8. Customize your plot so that the colors of the country names match the colors in our UK and Ireland map and table at start of this document.


```{r}
# Create a data frame for plotting
df <- as.data.frame(pca$x)
df$Country <- rownames(df)

my_cols <- c("Wales"="red", "England"="orange", "Scotland"="blue", "N.Ireland"="darkgreen")

# Plot PC1 vs PC2 with ggplot
ggplot(df, aes(x = PC1, y = PC2, label = Country, color = Country)) +
  geom_point(size = 3, col="grey") +
  geom_text(vjust = -0.5) +
  scale_color_manual(values = my_cols) +
  xlim(-270, 500) +
  xlab("PC1") +
  ylab("PC2") +
  theme_bw()
```

```{r}
v <- round( pca$sdev^2/sum(pca$sdev^2) * 100 )
v
```

```{r}
z <- summary(pca)
z$importance
```

> Q. How much variance is captured in the first PC? 67.4%

> Q. How many PCs do I need to capture at least 90% of the total variance in the dataset? Two PCs capture 96.5% of the total variance.

> Q. Plot our main PCA result. Folks can call this different things depending on their field of study e.g. "PC plot", "ordienation plot", "Score plot", "PC1 vs. PC2 plot"...

```{r}
# Create scree plot with ggplot
variance_df <- data.frame(
  PC = factor(paste0("PC", 1:length(v)), levels = paste0("PC", 1:length(v))),
  Variance = v
)

ggplot(variance_df) +
  aes(x = PC, y = Variance) +
  geom_col(fill = "steelblue") +
  xlab("Principal Component") +
  ylab("Percent Variation") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 0))
```

```{r}
attributes(pca)
```
To generate our PCA score plot we want the `pca$x` component of the result object.

```{r}
pca$x
```
```{r}
plot(pca$x[,1], pca$x[,2])
```
```{r}
library(ggplot2)

ggplot(pca$x) +
  aes(PC1, PC2) +
  geom_point(col=my_cols)
```

```{r}
my_cols <- c("orange", "red", "blue", "darkgreen")
plot(pca$x[,1], pca$x[,2], col=my_cols, pch=16)
```

## Digging deeper (variable loadings)

How do the original variables (i.e., the 17 different foods) contribute to our new PCs?

```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
ggplot(pca$rotation) +
  aes(x = PC1, 
      y = reorder(rownames(pca$rotation), PC1)) +
  geom_col(fill = "steelblue") +
  xlab("PC1 Loading Score") +
  ylab("") +
  theme_bw() +
  theme(axis.text.y = element_text(size = 9))
```
> Q9: Generate a similar ‘loadings plot’ for PC2. What two food groups feature prominantely and what does PC2 maninly tell us about? Soft drinks and alcoholic drinks.

```{r}
ggplot(pca$rotation) +
  aes(x = PC2, 
      y = reorder(rownames(pca$rotation), PC2)) +
  geom_col(fill = "lightgreen") +
  xlab("PC2 Loading Score") +
  ylab("") +
  theme_bw() +
  theme(axis.text.y = element_text(size = 9))
```
## PCA of RNA-seq data 

```{r}
url2 <- "https://tinyurl.com/expression-CSV"
rna.data <- read.csv(url2, row.names=1)
head(rna.data)
```

```{r}
nrow(rna.data)
ncol(rna.data)
```


> Q10: How many genes and samples are in this data set? How many PCs do you think it will take to have a useful overview of this data set (see below)? 100 genes and 10 samples. Based on the PC1 vs PC2 plot, the first two PCs already seem to separate the two samples meaningfully.

```{r}
## Again we have to take the transpose of our data 
pca <- prcomp(t(rna.data), scale=TRUE)

# Create data frame for plotting
df <- as.data.frame(pca$x)
df$Sample <- rownames(df)

## Plot with ggplot
ggplot(df) +
  aes(x = PC1, y = PC2, label = Sample) +
  geom_point(size = 3) +
  geom_text(vjust = -0.5, size = 3) +
  xlab("PC1") +
  ylab("PC2") +
  theme_bw()
```
```{r}
summary(pca)
```


```{r}
# Calculate variance explained
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)

# Create scree plot data
scree_df <- data.frame(
  PC = factor(paste0("PC", 1:10), levels = paste0("PC", 1:10)),
  Variance = pca.var[1:10]
)

ggplot(scree_df) +
  aes(x = PC, y = Variance) +
  geom_col(fill = "steelblue") +
  ggtitle("Quick scree plot") +
  xlab("Principal Component") +
  ylab("Variance") +
  theme_bw()
```
```{r}
## Percent variance is often more informative to look at 
pca.var.per
```

```{r}
# Create percent variance scree plot
scree_pct_df <- data.frame(
  PC = factor(paste0("PC", 1:10), levels = paste0("PC", 1:10)),
  PercentVariation = pca.var.per[1:10]
)

ggplot(scree_pct_df) +
  aes(x = PC, y = PercentVariation) +
  geom_col(fill = "steelblue") +
  ggtitle("Scree Plot") +
  xlab("Principal Component") +
  ylab("Percent Variation") +
  theme_bw()
```

```{r}
## A vector of colors for wt and ko samples
colvec <- colnames(rna.data)
colvec[grep("wt", colvec)] <- "red"
colvec[grep("ko", colvec)] <- "blue"

# Add condition to data frame
df$condition <- substr(df$Sample, 1, 2)
df$color <- colvec

ggplot(df) +
  aes(x = PC1, y = PC2, color = color, label = Sample) +
  geom_point(size = 3) +
  geom_text(vjust = -0.5, hjust = 0.5, show.legend = FALSE) +
  scale_color_identity() +
  xlab(paste0("PC1 (", pca.var.per[1], "%)")) +
  ylab(paste0("PC2 (", pca.var.per[2], "%)")) +
  theme_bw()
```


